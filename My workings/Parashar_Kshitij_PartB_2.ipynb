{"cells":[{"cell_type":"markdown","metadata":{"id":"NFVxWZGJxprU"},"source":["# CS4001/4042 Assignment 1, Part B, Q2\n","In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"EycCozG06Duu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697283532518,"user_tz":-480,"elapsed":10848,"user":{"displayName":"Kshitij Parashar","userId":"09154330883446617085"}},"outputId":"d7a8207b-a359-4c4d-c3fb-3c240685b77b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-widedeep\n","  Downloading pytorch_widedeep-1.3.2-py3-none-any.whl (21.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.5.3)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.23.5)\n","Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.11.3)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.2.2)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.3.2)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (3.6.1)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.8.0.76)\n","Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (0.5.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (0.15.2+cu118)\n","Collecting einops (from pytorch-widedeep)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.15.0)\n","Collecting torchmetrics (from pytorch-widedeep)\n","  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (9.0.0)\n","Collecting fastparquet>=0.8.1 (from pytorch-widedeep)\n","  Downloading fastparquet-2023.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cramjam>=2.3 (from fastparquet>=0.8.1->pytorch-widedeep)\n","  Downloading cramjam-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2023.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet>=0.8.1->pytorch-widedeep) (23.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3.post1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.2.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pytorch-widedeep) (6.4.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (0.10.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (67.7.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (3.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch-widedeep) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch-widedeep) (17.0.2)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics->pytorch-widedeep)\n","  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->pytorch-widedeep) (9.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy->pytorch-widedeep) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->pytorch-widedeep) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch-widedeep) (1.3.0)\n","Installing collected packages: lightning-utilities, einops, cramjam, fastparquet, torchmetrics, pytorch-widedeep\n","Successfully installed cramjam-2.7.0 einops-0.7.0 fastparquet-2023.8.0 lightning-utilities-0.9.0 pytorch-widedeep-1.3.2 torchmetrics-1.2.0\n"]}],"source":["!pip install pytorch-widedeep"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"lq0elU0J53Yo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697283557627,"user_tz":-480,"elapsed":11896,"user":{"displayName":"Kshitij Parashar","userId":"09154330883446617085"}},"outputId":"7091cc25-09f4-4dad-a613-1382a48fc0e8"},"outputs":[{"output_type":"stream","name":"stderr","text":["<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"]}],"source":["SEED = 42\n","\n","import os\n","\n","import random\n","random.seed(SEED)\n","\n","import numpy as np\n","np.random.seed(SEED)\n","\n","import pandas as pd\n","\n","from pytorch_widedeep.preprocessing import TabPreprocessor\n","from pytorch_widedeep.models import TabMlp, WideDeep\n","from pytorch_widedeep import Trainer\n","from pytorch_widedeep.metrics import R2Score"]},{"cell_type":"markdown","metadata":{"id":"aU3xdVpwzuLx"},"source":[">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQxlocBN-8I3","executionInfo":{"status":"ok","timestamp":1697283580260,"user_tz":-480,"elapsed":20036,"user":{"displayName":"Kshitij Parashar","userId":"09154330883446617085"}},"outputId":"b3a94957-26c0-43ad-b470-84390c03a4f6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pexpect/popen_spawn.py:60: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n","  self._read_thread.setDaemon(True)\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_oYG6lNIh7Mp","executionInfo":{"status":"ok","timestamp":1697283605550,"user_tz":-480,"elapsed":748,"user":{"displayName":"Kshitij Parashar","userId":"09154330883446617085"}}},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/hdb_price_prediction.csv')\n","\n","# Splitting the data into train, validation, and test sets\n","train_data = df[df['year'] <= 2020]\n","test_data = df[df['year'] >= 2021]"]},{"cell_type":"markdown","metadata":{"id":"m_q9PoR50JAA"},"source":[">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n","https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n","* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n","features and the categorical features. Use this component to transform the training dataset.\n","* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n","* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"]},{"cell_type":"code","source":[],"metadata":{"id":"vFEFOJYe2wDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ZBY1iqUXtYWn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697283919928,"user_tz":-480,"elapsed":322,"user":{"displayName":"Kshitij Parashar","userId":"09154330883446617085"}},"outputId":"17fc51b7-a796-4d32-ad9c-1d3fbe0efd6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'month': 0, 'year': 1, 'town': 2, 'full_address': 3, 'nearest_stn': 4, 'dist_to_nearest_stn': 5, 'dist_to_dhoby': 6, 'degree_centrality': 7, 'eigenvector_centrality': 8, 'flat_model_type': 9, 'remaining_lease_years': 10, 'floor_area_sqm': 11, 'storey_range': 12, 'resale_price': 13}\n","TabMlp(\n","  (cat_and_cont_embed): DiffSizeCatAndContEmbeddings(\n","    (cat_embed): DiffSizeCatEmbeddings(\n","      (embed_layers): ModuleDict(\n","        (emb_layer_month): Embedding(13, 12, padding_idx=0)\n","        (emb_layer_town): Embedding(27, 26, padding_idx=0)\n","        (emb_layer_flat_model_type): Embedding(44, 43, padding_idx=0)\n","        (emb_layer_storey_range): Embedding(18, 17, padding_idx=0)\n","      )\n","      (embedding_dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (cont_norm): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (encoder): MLP(\n","    (mlp): Sequential(\n","      (dense_layer_0): Sequential(\n","        (0): Dropout(p=0.1, inplace=False)\n","        (1): Linear(in_features=104, out_features=200, bias=True)\n","        (2): ReLU(inplace=True)\n","      )\n","      (dense_layer_1): Sequential(\n","        (0): Dropout(p=0.1, inplace=False)\n","        (1): Linear(in_features=200, out_features=100, bias=True)\n","        (2): ReLU(inplace=True)\n","      )\n","    )\n","  )\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["import torch\n","from pytorch_widedeep.initializers import Normal\n","from pytorch_widedeep.callbacks import EarlyStopping\n","\n","target = [\"resale_price\"]\n","\n","# lists for continuous and categorical variables\n","continuous_var =  [\"dist_to_nearest_stn\", \"dist_to_dhoby\", \"degree_centrality\", \"eigenvector_centrality\", \"remaining_lease_years\", \"floor_area_sqm\"]\n","categorical_var = [\"month\", \"town\", \"flat_model_type\", \"storey_range\"]\n","\n","categorical_tup = []\n","for variable in categorical_var:\n","  categorical_tup.append( ( variable,int(train_data[variable].unique().shape[0])))\n","\n","preprocessor = TabPreprocessor(\n","    cat_embed_cols=categorical_tup,\n","    continuous_cols=continuous_var,\n","    cols_to_scale=continuous_var\n",")\n","\n","X_tab = preprocessor.fit_transform(train_data)\n","\n","target = train_data[target].values\n","\n","col_names = list(train_data.columns)\n","col_idx = {i:j for j,i in enumerate(col_names)}\n","print(col_idx)\n","\n","hidden_dimenions = [200, 100]\n","\n","deep_tabular_model = TabMlp(\n","    column_idx=preprocessor.column_idx,\n","    cat_embed_input=preprocessor.cat_embed_input,\n","    continuous_cols=continuous_var,\n","    mlp_hidden_dims=hidden_dimenions\n","    )\n","\n","print(deep_tabular_model)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rnA2nT4d2Pn2","executionInfo":{"status":"ok","timestamp":1697285292220,"user_tz":-480,"elapsed":1229759,"user":{"displayName":"Kshitij Parashar","userId":"09154330883446617085"}},"outputId":"c5808683-bffa-4ab5-f49e-ea5366005594"},"outputs":[{"output_type":"stream","name":"stderr","text":["epoch 1: 100%|██████████| 1366/1366 [00:12<00:00, 108.80it/s, loss=9.34e+4, metrics={'r2': 0.3578}]\n","/usr/local/lib/python3.10/dist-packages/pytorch_widedeep/callbacks.py:680: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: train_loss,train_r2\n","  warnings.warn(\n","epoch 2: 100%|██████████| 1366/1366 [00:11<00:00, 117.13it/s, loss=6.08e+4, metrics={'r2': 0.8356}]\n","epoch 3: 100%|██████████| 1366/1366 [00:15<00:00, 88.35it/s, loss=5.83e+4, metrics={'r2': 0.8477}]\n","epoch 4: 100%|██████████| 1366/1366 [00:13<00:00, 104.90it/s, loss=5.73e+4, metrics={'r2': 0.8513}]\n","epoch 5: 100%|██████████| 1366/1366 [00:11<00:00, 116.33it/s, loss=5.63e+4, metrics={'r2': 0.8579}]\n","epoch 6: 100%|██████████| 1366/1366 [00:12<00:00, 111.23it/s, loss=5.49e+4, metrics={'r2': 0.8653}]\n","epoch 7: 100%|██████████| 1366/1366 [00:12<00:00, 110.70it/s, loss=5.42e+4, metrics={'r2': 0.8684}]\n","epoch 8: 100%|██████████| 1366/1366 [00:12<00:00, 110.12it/s, loss=5.42e+4, metrics={'r2': 0.8676}]\n","epoch 9: 100%|██████████| 1366/1366 [00:11<00:00, 115.83it/s, loss=5.28e+4, metrics={'r2': 0.8746}]\n","epoch 10: 100%|██████████| 1366/1366 [00:11<00:00, 115.02it/s, loss=5.16e+4, metrics={'r2': 0.8806}]\n","epoch 11: 100%|██████████| 1366/1366 [00:11<00:00, 116.21it/s, loss=5.16e+4, metrics={'r2': 0.8804}]\n","epoch 12: 100%|██████████| 1366/1366 [00:11<00:00, 115.94it/s, loss=5.1e+4, metrics={'r2': 0.8833}]\n","epoch 13: 100%|██████████| 1366/1366 [00:12<00:00, 113.02it/s, loss=5.06e+4, metrics={'r2': 0.8846}]\n","epoch 14: 100%|██████████| 1366/1366 [00:11<00:00, 117.61it/s, loss=4.96e+4, metrics={'r2': 0.8888}]\n","epoch 15: 100%|██████████| 1366/1366 [00:11<00:00, 119.31it/s, loss=4.9e+4, metrics={'r2': 0.8918}]\n","epoch 16: 100%|██████████| 1366/1366 [00:11<00:00, 117.22it/s, loss=4.89e+4, metrics={'r2': 0.8922}]\n","epoch 17: 100%|██████████| 1366/1366 [00:11<00:00, 114.24it/s, loss=4.84e+4, metrics={'r2': 0.8947}]\n","epoch 18: 100%|██████████| 1366/1366 [00:11<00:00, 116.57it/s, loss=4.77e+4, metrics={'r2': 0.8972}]\n","epoch 19: 100%|██████████| 1366/1366 [00:11<00:00, 120.60it/s, loss=4.72e+4, metrics={'r2': 0.8997}]\n","epoch 20: 100%|██████████| 1366/1366 [00:12<00:00, 109.68it/s, loss=4.66e+4, metrics={'r2': 0.9019}]\n","epoch 21: 100%|██████████| 1366/1366 [00:11<00:00, 116.15it/s, loss=4.69e+4, metrics={'r2': 0.9006}]\n","epoch 22: 100%|██████████| 1366/1366 [00:11<00:00, 118.57it/s, loss=4.64e+4, metrics={'r2': 0.9031}]\n","epoch 23: 100%|██████████| 1366/1366 [00:11<00:00, 116.59it/s, loss=4.59e+4, metrics={'r2': 0.9051}]\n","epoch 24: 100%|██████████| 1366/1366 [00:11<00:00, 114.69it/s, loss=4.57e+4, metrics={'r2': 0.9056}]\n","epoch 25: 100%|██████████| 1366/1366 [00:11<00:00, 116.27it/s, loss=4.54e+4, metrics={'r2': 0.9067}]\n","epoch 26: 100%|██████████| 1366/1366 [00:11<00:00, 116.09it/s, loss=4.55e+4, metrics={'r2': 0.9067}]\n","epoch 27: 100%|██████████| 1366/1366 [00:11<00:00, 116.60it/s, loss=4.5e+4, metrics={'r2': 0.9083}]\n","epoch 28: 100%|██████████| 1366/1366 [00:11<00:00, 116.58it/s, loss=4.49e+4, metrics={'r2': 0.9091}]\n","epoch 29: 100%|██████████| 1366/1366 [00:11<00:00, 119.66it/s, loss=4.48e+4, metrics={'r2': 0.9094}]\n","epoch 30: 100%|██████████| 1366/1366 [00:11<00:00, 114.31it/s, loss=4.5e+4, metrics={'r2': 0.9084}]\n","epoch 31: 100%|██████████| 1366/1366 [00:11<00:00, 116.28it/s, loss=4.46e+4, metrics={'r2': 0.9104}]\n","epoch 32: 100%|██████████| 1366/1366 [00:11<00:00, 122.79it/s, loss=4.46e+4, metrics={'r2': 0.9105}]\n","epoch 33: 100%|██████████| 1366/1366 [00:11<00:00, 116.91it/s, loss=4.43e+4, metrics={'r2': 0.9113}]\n","epoch 34: 100%|██████████| 1366/1366 [00:12<00:00, 112.47it/s, loss=4.44e+4, metrics={'r2': 0.9108}]\n","epoch 35: 100%|██████████| 1366/1366 [00:11<00:00, 116.59it/s, loss=4.42e+4, metrics={'r2': 0.912}]\n","epoch 36: 100%|██████████| 1366/1366 [00:11<00:00, 114.55it/s, loss=4.43e+4, metrics={'r2': 0.9116}]\n","epoch 37: 100%|██████████| 1366/1366 [00:11<00:00, 114.31it/s, loss=4.39e+4, metrics={'r2': 0.9129}]\n","epoch 38: 100%|██████████| 1366/1366 [00:11<00:00, 118.84it/s, loss=4.42e+4, metrics={'r2': 0.9118}]\n","epoch 39: 100%|██████████| 1366/1366 [00:11<00:00, 116.09it/s, loss=4.41e+4, metrics={'r2': 0.9124}]\n","epoch 40: 100%|██████████| 1366/1366 [00:11<00:00, 114.29it/s, loss=4.38e+4, metrics={'r2': 0.9132}]\n","epoch 41: 100%|██████████| 1366/1366 [00:11<00:00, 120.19it/s, loss=4.4e+4, metrics={'r2': 0.9125}]\n","epoch 42: 100%|██████████| 1366/1366 [00:11<00:00, 114.76it/s, loss=4.39e+4, metrics={'r2': 0.9131}]\n","epoch 43: 100%|██████████| 1366/1366 [00:12<00:00, 111.35it/s, loss=4.38e+4, metrics={'r2': 0.9133}]\n","epoch 44: 100%|██████████| 1366/1366 [00:11<00:00, 117.92it/s, loss=4.37e+4, metrics={'r2': 0.9136}]\n","epoch 45: 100%|██████████| 1366/1366 [00:11<00:00, 114.81it/s, loss=4.35e+4, metrics={'r2': 0.9145}]\n","epoch 46: 100%|██████████| 1366/1366 [00:12<00:00, 112.90it/s, loss=4.39e+4, metrics={'r2': 0.9128}]\n","epoch 47: 100%|██████████| 1366/1366 [00:11<00:00, 117.55it/s, loss=4.4e+4, metrics={'r2': 0.9127}]\n","epoch 48: 100%|██████████| 1366/1366 [00:12<00:00, 107.06it/s, loss=4.35e+4, metrics={'r2': 0.9145}]\n","epoch 49: 100%|██████████| 1366/1366 [00:11<00:00, 115.13it/s, loss=4.35e+4, metrics={'r2': 0.9143}]\n","epoch 50: 100%|██████████| 1366/1366 [00:11<00:00, 118.50it/s, loss=4.34e+4, metrics={'r2': 0.9149}]\n","epoch 51: 100%|██████████| 1366/1366 [00:11<00:00, 114.45it/s, loss=4.34e+4, metrics={'r2': 0.9149}]\n","epoch 52: 100%|██████████| 1366/1366 [00:12<00:00, 111.64it/s, loss=4.35e+4, metrics={'r2': 0.9144}]\n","epoch 53: 100%|██████████| 1366/1366 [00:11<00:00, 118.71it/s, loss=4.35e+4, metrics={'r2': 0.9145}]\n","epoch 54: 100%|██████████| 1366/1366 [00:11<00:00, 116.95it/s, loss=4.33e+4, metrics={'r2': 0.9151}]\n","epoch 55: 100%|██████████| 1366/1366 [00:11<00:00, 113.87it/s, loss=4.34e+4, metrics={'r2': 0.915}]\n","epoch 56: 100%|██████████| 1366/1366 [00:11<00:00, 115.61it/s, loss=4.31e+4, metrics={'r2': 0.9159}]\n","epoch 57: 100%|██████████| 1366/1366 [00:11<00:00, 114.24it/s, loss=4.33e+4, metrics={'r2': 0.9154}]\n","epoch 58: 100%|██████████| 1366/1366 [00:11<00:00, 114.32it/s, loss=4.34e+4, metrics={'r2': 0.9152}]\n","epoch 59: 100%|██████████| 1366/1366 [00:11<00:00, 114.92it/s, loss=4.31e+4, metrics={'r2': 0.9159}]\n","epoch 60: 100%|██████████| 1366/1366 [00:11<00:00, 115.87it/s, loss=4.29e+4, metrics={'r2': 0.917}]\n","epoch 61: 100%|██████████| 1366/1366 [00:12<00:00, 106.87it/s, loss=4.31e+4, metrics={'r2': 0.9163}]\n","epoch 62: 100%|██████████| 1366/1366 [00:12<00:00, 111.37it/s, loss=4.3e+4, metrics={'r2': 0.9165}]\n","epoch 63: 100%|██████████| 1366/1366 [00:12<00:00, 112.21it/s, loss=4.33e+4, metrics={'r2': 0.9156}]\n","epoch 64: 100%|██████████| 1366/1366 [00:12<00:00, 109.22it/s, loss=4.29e+4, metrics={'r2': 0.917}]\n","epoch 65: 100%|██████████| 1366/1366 [00:12<00:00, 106.25it/s, loss=4.27e+4, metrics={'r2': 0.9177}]\n","epoch 66: 100%|██████████| 1366/1366 [00:12<00:00, 112.77it/s, loss=4.32e+4, metrics={'r2': 0.9157}]\n","epoch 67: 100%|██████████| 1366/1366 [00:12<00:00, 107.42it/s, loss=4.29e+4, metrics={'r2': 0.9169}]\n","epoch 68: 100%|██████████| 1366/1366 [00:12<00:00, 107.10it/s, loss=4.29e+4, metrics={'r2': 0.9169}]\n","epoch 69: 100%|██████████| 1366/1366 [00:12<00:00, 106.68it/s, loss=4.29e+4, metrics={'r2': 0.9169}]\n","epoch 70: 100%|██████████| 1366/1366 [00:12<00:00, 110.08it/s, loss=4.25e+4, metrics={'r2': 0.9183}]\n","epoch 71: 100%|██████████| 1366/1366 [00:12<00:00, 109.39it/s, loss=4.27e+4, metrics={'r2': 0.9176}]\n","epoch 72: 100%|██████████| 1366/1366 [00:12<00:00, 106.91it/s, loss=4.26e+4, metrics={'r2': 0.918}]\n","epoch 73: 100%|██████████| 1366/1366 [00:12<00:00, 108.20it/s, loss=4.28e+4, metrics={'r2': 0.9173}]\n","epoch 74: 100%|██████████| 1366/1366 [00:13<00:00, 100.96it/s, loss=4.27e+4, metrics={'r2': 0.9177}]\n","epoch 75: 100%|██████████| 1366/1366 [00:12<00:00, 109.07it/s, loss=4.28e+4, metrics={'r2': 0.9172}]\n","epoch 76: 100%|██████████| 1366/1366 [00:12<00:00, 108.45it/s, loss=4.28e+4, metrics={'r2': 0.9174}]\n","epoch 77: 100%|██████████| 1366/1366 [00:12<00:00, 108.64it/s, loss=4.26e+4, metrics={'r2': 0.9179}]\n","epoch 78: 100%|██████████| 1366/1366 [00:12<00:00, 112.95it/s, loss=4.27e+4, metrics={'r2': 0.9178}]\n","epoch 79: 100%|██████████| 1366/1366 [00:12<00:00, 109.73it/s, loss=4.25e+4, metrics={'r2': 0.9184}]\n","epoch 80: 100%|██████████| 1366/1366 [00:12<00:00, 108.42it/s, loss=4.27e+4, metrics={'r2': 0.9177}]\n","epoch 81: 100%|██████████| 1366/1366 [00:12<00:00, 105.40it/s, loss=4.25e+4, metrics={'r2': 0.9186}]\n","epoch 82: 100%|██████████| 1366/1366 [00:12<00:00, 111.78it/s, loss=4.27e+4, metrics={'r2': 0.9178}]\n","epoch 83: 100%|██████████| 1366/1366 [00:12<00:00, 107.06it/s, loss=4.25e+4, metrics={'r2': 0.9185}]\n","epoch 84: 100%|██████████| 1366/1366 [00:12<00:00, 106.92it/s, loss=4.21e+4, metrics={'r2': 0.9199}]\n","epoch 85: 100%|██████████| 1366/1366 [00:12<00:00, 110.26it/s, loss=4.22e+4, metrics={'r2': 0.9194}]\n","epoch 86: 100%|██████████| 1366/1366 [00:12<00:00, 105.83it/s, loss=4.21e+4, metrics={'r2': 0.9203}]\n","epoch 87: 100%|██████████| 1366/1366 [00:13<00:00, 101.41it/s, loss=4.22e+4, metrics={'r2': 0.9196}]\n","epoch 88: 100%|██████████| 1366/1366 [00:13<00:00, 102.12it/s, loss=4.24e+4, metrics={'r2': 0.9187}]\n","epoch 89: 100%|██████████| 1366/1366 [00:12<00:00, 109.79it/s, loss=4.2e+4, metrics={'r2': 0.9202}]\n","epoch 90: 100%|██████████| 1366/1366 [00:12<00:00, 108.14it/s, loss=4.24e+4, metrics={'r2': 0.9188}]\n","epoch 91: 100%|██████████| 1366/1366 [00:12<00:00, 105.52it/s, loss=4.22e+4, metrics={'r2': 0.9195}]\n","epoch 92: 100%|██████████| 1366/1366 [00:12<00:00, 105.95it/s, loss=4.2e+4, metrics={'r2': 0.9204}]\n","epoch 93: 100%|██████████| 1366/1366 [00:12<00:00, 109.44it/s, loss=4.18e+4, metrics={'r2': 0.9211}]\n","epoch 94: 100%|██████████| 1366/1366 [00:12<00:00, 106.30it/s, loss=4.21e+4, metrics={'r2': 0.9199}]\n","epoch 95: 100%|██████████| 1366/1366 [00:12<00:00, 106.04it/s, loss=4.22e+4, metrics={'r2': 0.9196}]\n","epoch 96: 100%|██████████| 1366/1366 [00:12<00:00, 108.42it/s, loss=4.2e+4, metrics={'r2': 0.9202}]\n","epoch 97: 100%|██████████| 1366/1366 [00:12<00:00, 109.51it/s, loss=4.19e+4, metrics={'r2': 0.9207}]\n","epoch 98: 100%|██████████| 1366/1366 [00:12<00:00, 110.10it/s, loss=4.19e+4, metrics={'r2': 0.9208}]\n","epoch 99: 100%|██████████| 1366/1366 [00:13<00:00, 103.98it/s, loss=4.22e+4, metrics={'r2': 0.9197}]\n","epoch 100: 100%|██████████| 1366/1366 [00:12<00:00, 107.89it/s, loss=4.2e+4, metrics={'r2': 0.9202}]\n","predict: 100%|██████████| 1366/1366 [00:04<00:00, 278.54it/s]\n"]}],"source":["deep_optimizer = torch.optim.Adam(deep_tabular_model.parameters(), lr=0.01)     # Defining the optimizer for the deep tabular model\n","callbacks = [EarlyStopping]                                                     # Defining callbacks for early stopping\n","optimizer = {\"deeptabular\": deep_optimizer}                                     # Creating an optimizer dictionary with the deep tabular model optimizer\n","metric = [R2Score]                                                              # Defining the metric for model evaluation\n","initializer = {\"deeptabular\": Normal}                                           # Initializing weights and biases using the Normal distribution\n","\n","\n","model = WideDeep(wide=None,\n","                deeptabular=deep_tabular_model,\n","                deeptext=None,\n","                deepimage=None,\n","                deephead=None,\n","                head_hidden_dims=None,\n","                head_activation='relu',\n","                head_dropout=0.1,\n","                head_batchnorm=False,\n","                head_batchnorm_last=False,\n","                head_linear_first=True,\n","                enforce_positive=False,\n","                enforce_positive_activation='softplus',\n","                pred_dim=1,\n","                with_fds=False)\n","\n","trainer = Trainer(model=model,\n","                  objective=\"root_mean_squared_error\",\n","                  optimizers=optimizer,\n","                  lr_schedulers=None,\n","                  initializers=initializer,\n","                  callbacks=callbacks,\n","                  metrics=metric,\n","                  verbose=1,\n","                  num_workers = 0)\n","\n","trainer.fit(X_tab=X_tab,\n","            n_epochs=100,\n","            batch_size=64,\n","            target = target)\n","\n","preds = trainer.predict(X_tab=X_tab)"]},{"cell_type":"markdown","metadata":{"id":"V46s-MdM0y5c"},"source":[">Report the test RMSE and the test R2 value that you obtained."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"KAhAgvMC07g6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697285493346,"user_tz":-480,"elapsed":4120,"user":{"displayName":"Kshitij Parashar","userId":"09154330883446617085"}},"outputId":"e47605fc-6696-469e-f55c-d24c0a5597cb"},"outputs":[{"output_type":"stream","name":"stderr","text":["predict: 100%|██████████| 1128/1128 [00:03<00:00, 310.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test RMSE: 100919.06071344888\n","Test R2: 0.6441680590167506\n"]}],"source":["from sklearn.metrics import mean_squared_error, r2_score\n","import numpy as np\n","\n","# Preprocess the test data\n","X_test = preprocessor.transform(test_data)\n","\n","# Extract the true target values from the test data\n","target_test = test_data['resale_price'].values\n","\n","# Make predictions on the test data\n","test_preds = trainer.predict(X_tab=X_test)\n","\n","# Calculate RMSE and R2 for the test predictions\n","test_rmse = np.sqrt(mean_squared_error(target_test, test_preds))\n","test_r2 = r2_score(target_test, test_preds)\n","\n","print(\"Test RMSE:\", test_rmse)\n","print(\"Test R2:\", test_r2)"]},{"cell_type":"code","source":[],"metadata":{"id":"JLHV3iiZ9EfS"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1m0qBKAIeU__gJkzo57cH7dJc3uMjq5He","timestamp":1697283479269}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}